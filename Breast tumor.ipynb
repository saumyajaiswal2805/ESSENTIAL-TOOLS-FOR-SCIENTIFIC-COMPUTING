{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6780ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3da33e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast tumor dataset into a Pandas DataFrame\n",
    "data = pd.read_csv(r'E:\\MLBS\\Project Breast Cancer\\breast cancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "596fa2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# explore the dataset using functions like 'info()', 'describe()', and 'head()'\n",
    "print(data.info())  # Display basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e962ae5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
      "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
      "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
      "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
      "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
      "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
      "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
      "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
      "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
      "\n",
      "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "count       569.000000        569.000000      569.000000           569.000000   \n",
      "mean          0.096360          0.104341        0.088799             0.048919   \n",
      "std           0.014064          0.052813        0.079720             0.038803   \n",
      "min           0.052630          0.019380        0.000000             0.000000   \n",
      "25%           0.086370          0.064920        0.029560             0.020310   \n",
      "50%           0.095870          0.092630        0.061540             0.033500   \n",
      "75%           0.105300          0.130400        0.130700             0.074000   \n",
      "max           0.163400          0.345400        0.426800             0.201200   \n",
      "\n",
      "       symmetry_mean  ...  texture_worst  perimeter_worst   area_worst  \\\n",
      "count     569.000000  ...     569.000000       569.000000   569.000000   \n",
      "mean        0.181162  ...      25.677223       107.261213   880.583128   \n",
      "std         0.027414  ...       6.146258        33.602542   569.356993   \n",
      "min         0.106000  ...      12.020000        50.410000   185.200000   \n",
      "25%         0.161900  ...      21.080000        84.110000   515.300000   \n",
      "50%         0.179200  ...      25.410000        97.660000   686.500000   \n",
      "75%         0.195700  ...      29.720000       125.400000  1084.000000   \n",
      "max         0.304000  ...      49.540000       251.200000  4254.000000   \n",
      "\n",
      "       smoothness_worst  compactness_worst  concavity_worst  \\\n",
      "count        569.000000         569.000000       569.000000   \n",
      "mean           0.132369           0.254265         0.272188   \n",
      "std            0.022832           0.157336         0.208624   \n",
      "min            0.071170           0.027290         0.000000   \n",
      "25%            0.116600           0.147200         0.114500   \n",
      "50%            0.131300           0.211900         0.226700   \n",
      "75%            0.146000           0.339100         0.382900   \n",
      "max            0.222600           1.058000         1.252000   \n",
      "\n",
      "       concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
      "count            569.000000      569.000000               569.000000   \n",
      "mean               0.114606        0.290076                 0.083946   \n",
      "std                0.065732        0.061867                 0.018061   \n",
      "min                0.000000        0.156500                 0.055040   \n",
      "25%                0.064930        0.250400                 0.071460   \n",
      "50%                0.099930        0.282200                 0.080040   \n",
      "75%                0.161400        0.317900                 0.092080   \n",
      "max                0.291000        0.663800                 0.207500   \n",
      "\n",
      "       Unnamed: 32  \n",
      "count          0.0  \n",
      "mean           NaN  \n",
      "std            NaN  \n",
      "min            NaN  \n",
      "25%            NaN  \n",
      "50%            NaN  \n",
      "75%            NaN  \n",
      "max            NaN  \n",
      "\n",
      "[8 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.describe())  # Display statistics of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee8628be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0    842302         M        17.99         10.38          122.80     1001.0   \n",
      "1    842517         M        20.57         17.77          132.90     1326.0   \n",
      "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
      "3  84348301         M        11.42         20.38           77.58      386.1   \n",
      "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "3          0.14250           0.28390          0.2414              0.10520   \n",
      "4          0.10030           0.13280          0.1980              0.10430   \n",
      "\n",
      "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
      "0  ...          17.33           184.60      2019.0            0.1622   \n",
      "1  ...          23.41           158.80      1956.0            0.1238   \n",
      "2  ...          25.53           152.50      1709.0            0.1444   \n",
      "3  ...          26.50            98.87       567.7            0.2098   \n",
      "4  ...          16.67           152.20      1575.0            0.1374   \n",
      "\n",
      "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "0             0.6656           0.7119                0.2654          0.4601   \n",
      "1             0.1866           0.2416                0.1860          0.2750   \n",
      "2             0.4245           0.4504                0.2430          0.3613   \n",
      "3             0.8663           0.6869                0.2575          0.6638   \n",
      "4             0.2050           0.4000                0.1625          0.2364   \n",
      "\n",
      "   fractal_dimension_worst  Unnamed: 32  \n",
      "0                  0.11890          NaN  \n",
      "1                  0.08902          NaN  \n",
      "2                  0.08758          NaN  \n",
      "3                  0.17300          NaN  \n",
      "4                  0.07678          NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.head())  # Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9281559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.drop(columns=['id', 'Unnamed: 32'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae1681eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagnosis                  0\n",
      "radius_mean                0\n",
      "texture_mean               0\n",
      "perimeter_mean             0\n",
      "area_mean                  0\n",
      "smoothness_mean            0\n",
      "compactness_mean           0\n",
      "concavity_mean             0\n",
      "concave points_mean        0\n",
      "symmetry_mean              0\n",
      "fractal_dimension_mean     0\n",
      "radius_se                  0\n",
      "texture_se                 0\n",
      "perimeter_se               0\n",
      "area_se                    0\n",
      "smoothness_se              0\n",
      "compactness_se             0\n",
      "concavity_se               0\n",
      "concave points_se          0\n",
      "symmetry_se                0\n",
      "fractal_dimension_se       0\n",
      "radius_worst               0\n",
      "texture_worst              0\n",
      "perimeter_worst            0\n",
      "area_worst                 0\n",
      "smoothness_worst           0\n",
      "compactness_worst          0\n",
      "concavity_worst            0\n",
      "concave points_worst       0\n",
      "symmetry_worst             0\n",
      "fractal_dimension_worst    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = dataset.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea5cc69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['diagnosis'] = dataset['diagnosis'].map({'M': 1, 'B': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a640228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "columns_to_scale = dataset.columns[:-1]  # Exclude the last column (diagnosis)\n",
    "\n",
    "# Apply the StandardScaler to the selected columns\n",
    "dataset[columns_to_scale] = scaler.fit_transform(dataset[columns_to_scale])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31b1eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and labels (y)\n",
    "X = dataset.drop(columns=['diagnosis'])\n",
    "y = dataset['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80a78d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (e.g., 80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "329cb926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / num_samples) * np.sum(y_predicted - y)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        return y_predicted\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_predicted = self.predict(X_test)\n",
    "        accuracy = np.mean(y_predicted == y_test)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2d339fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(self, y, y_predicted):\n",
    "    num_samples = len(y)\n",
    "    cost = - (1 / num_samples) * (np.dot(y, np.log(y_predicted)) + np.dot((1 - y), np.log(1 - y_predicted)))\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9efa8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the logistic regression model\n",
    "model = LogisticRegression(learning_rate=0.01, num_iterations=1000)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82f06f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = np.array(model.predict(X_test))\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "511875d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.37%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert y_pred_binary to integers if needed\n",
    "y_pred_binary = y_pred_binary.astype(int)\n",
    "\n",
    "# Convert y_test to integers\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "47535709",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test shape: (114,)\n",
      "y_pred_binary shape: (114,)\n",
      "y_test data type: int32\n",
      "y_pred_binary data type: int32\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"y_pred_binary shape: {y_pred_binary.shape}\")\n",
    "print(f\"y_test data type: {y_test.dtype}\")\n",
    "print(f\"y_pred_binary data type: {y_pred_binary.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "52ddbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Initialize an imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit the imputer on training data\n",
    "imputer.fit(X_train)\n",
    "\n",
    "# Transform both the training and test data with the imputer\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "142f2a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 96.49%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the SVM classifier with a linear kernel\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Train the SVM model on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate the accuracy of the SVM model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0198d66d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 1s 10ms/step - loss: 0.4428 - accuracy: 0.8527 - val_loss: 0.2094 - val_accuracy: 0.9649\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1814 - accuracy: 0.9495 - val_loss: 0.1131 - val_accuracy: 0.9737\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1191 - accuracy: 0.9604 - val_loss: 0.0835 - val_accuracy: 0.9825\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0913 - accuracy: 0.9692 - val_loss: 0.0753 - val_accuracy: 0.9737\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.9802 - val_loss: 0.0687 - val_accuracy: 0.9825\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9824 - val_loss: 0.0658 - val_accuracy: 0.9825\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9846 - val_loss: 0.0639 - val_accuracy: 0.9825\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9890 - val_loss: 0.0653 - val_accuracy: 0.9737\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9912 - val_loss: 0.0652 - val_accuracy: 0.9737\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0375 - accuracy: 0.9912 - val_loss: 0.0655 - val_accuracy: 0.9825\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "Neural Network Accuracy: 98.25%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "y_pred_nn = model.predict(X_test)\n",
    "\n",
    "accuracy_nn = accuracy_score(y_test, (y_pred_nn > 0.5).astype(int))\n",
    "print(f\"Neural Network Accuracy: {accuracy_nn * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d22cdb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 97.37%\n",
      "Logistic Regression Precision: 97.62%\n",
      "Logistic Regression Recall: 95.35%\n",
      "Logistic Regression F1-score: 96.47%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_logistic = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "# Calculate precision\n",
    "precision_logistic = precision_score(y_test, y_pred_binary,zero_division=1.0)\n",
    "\n",
    "# Calculate recall\n",
    "recall_logistic = recall_score(y_test, y_pred_binary)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1_logistic = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_logistic * 100:.2f}%\")\n",
    "print(f\"Logistic Regression Precision: {precision_logistic * 100:.2f}%\")\n",
    "print(f\"Logistic Regression Recall: {recall_logistic * 100:.2f}%\")\n",
    "print(f\"Logistic Regression F1-score: {f1_logistic * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d177fee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 96.49%\n",
      "SVM Precision: 93.33%\n",
      "SVM Recall: 97.67%\n",
      "SVM F1-score: 95.45%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "# Calculate precision\n",
    "precision_svm = precision_score(y_test, y_pred_svm)\n",
    "\n",
    "# Calculate recall\n",
    "recall_svm = recall_score(y_test, y_pred_svm)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1_svm = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"SVM Accuracy: {accuracy_svm * 100:.2f}%\")\n",
    "print(f\"SVM Precision: {precision_svm * 100:.2f}%\")\n",
    "print(f\"SVM Recall: {recall_svm * 100:.2f}%\")\n",
    "print(f\"SVM F1-score: {f1_svm * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "df23946a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Accuracy: 98.25%\n",
      "Neural Network Precision: 97.67%\n",
      "Neural Network Recall: 97.67%\n",
      "Neural Network F1-score: 97.67%\n"
     ]
    }
   ],
   "source": [
    "# Assuming y_pred_nn is your continuous predictions\n",
    "threshold = 0.5  # Set a threshold for binary classification\n",
    "\n",
    "# Binarize the predictions\n",
    "y_pred_nn_binary = (y_pred_nn > threshold).astype(int)\n",
    "\n",
    "# Now, you can calculate classification metrics with the binarized predictions\n",
    "accuracy_neural = accuracy_score(y_test, y_pred_nn_binary)\n",
    "precision_neural = precision_score(y_test, y_pred_nn_binary)\n",
    "recall_neural = recall_score(y_test, y_pred_nn_binary)\n",
    "f1_neural = f1_score(y_test, y_pred_nn_binary)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Neural Network Accuracy: {accuracy_neural * 100:.2f}%\")\n",
    "print(f\"Neural Network Precision: {precision_neural * 100:.2f}%\")\n",
    "print(f\"Neural Network Recall: {recall_neural * 100:.2f}%\")\n",
    "print(f\"Neural Network F1-score: {f1_neural * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3bdce782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Accuracy: 97.37%\n",
      "Precision: 97.62%\n",
      "Recall: 95.35%\n",
      "F1-score: 96.47%\n",
      "\n",
      "SVM:\n",
      "Accuracy: 96.49%\n",
      "Precision: 93.33%\n",
      "Recall: 97.67%\n",
      "F1-score: 95.45%\n",
      "\n",
      "Neural Network:\n",
      "Accuracy: 98.25%\n",
      "Precision: 97.67%\n",
      "Recall: 97.67%\n",
      "F1-score: 97.67%\n",
      "\n",
      "Neural Network performed the best.\n"
     ]
    }
   ],
   "source": [
    "# Results of Logistic Regression\n",
    "print(\"Logistic Regression:\")\n",
    "print(f\"Accuracy: {accuracy_logistic * 100:.2f}%\")\n",
    "print(f\"Precision: {precision_logistic * 100:.2f}%\")\n",
    "print(f\"Recall: {recall_logistic * 100:.2f}%\")\n",
    "print(f\"F1-score: {f1_logistic * 100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Results of SVM\n",
    "print(\"SVM:\")\n",
    "print(f\"Accuracy: {accuracy_svm * 100:.2f}%\")\n",
    "print(f\"Precision: {precision_svm * 100:.2f}%\")\n",
    "print(f\"Recall: {recall_svm * 100:.2f}%\")\n",
    "print(f\"F1-score: {f1_svm * 100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Results of Neural Network\n",
    "print(\"Neural Network:\")\n",
    "print(f\"Accuracy: {accuracy_neural * 100:.2f}%\")\n",
    "print(f\"Precision: {precision_neural * 100:.2f}%\")\n",
    "print(f\"Recall: {recall_neural * 100:.2f}%\")\n",
    "print(f\"F1-score: {f1_neural * 100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Conclusion\n",
    "if accuracy_logistic > accuracy_svm and accuracy_logistic > accuracy_neural:\n",
    "    print(\"Logistic Regression performed the best.\")\n",
    "elif accuracy_svm > accuracy_logistic and accuracy_svm > accuracy_neural:\n",
    "    print(\"SVM performed the best.\")\n",
    "else:\n",
    "    print(\"Neural Network performed the best.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb0859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4681645b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb635a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
